from torch import nn
import torch as t
import numpy as np
from typing import Callable, List, Any, Dict
from .training_helpers import Optimizers, Initializers, Activations
from .utils import StateParser, HeartsStateParser
from torch.nn import functional as F
from . import Agent

class ACAgent(nn.Module, Agent):
	def __init__(self,
				 full_deck,
				 state_size,
				 actor_learning_rate,
				 critic_learning_rate,
				 legal_actions_getter: Callable[[Any], List[Any]],
				 parser: StateParser =HeartsStateParser(),
				 gamma=0.99,
				 loss_decay = 0.99995,
				 critic_layers: List[int]=[],
				 actor_layers: List[int]=[],
				 rng: np.random.Generator =np.random.default_rng(2137),
				 actor_optimizer='adam',
				 critic_optimizer='adam',
				 actor_optimizer_params: Dict[str, Any] = {},
				 critic_optimizer_params: Dict[str, Any] = {},
				 activation='relu',
				 initializer='xavier_u',
				 initializer_params: Dict[str, Any] = {}):
     
		nn.Module.__init__(self)
		Agent.__init__(self, full_deck, (actor_learning_rate, critic_learning_rate), 0.0, gamma, legal_actions_getter, rng)
  
		self.parser = parser
		self.state_size = state_size
		self.action_size = 13 if full_deck else 6
		self.loss_decay = loss_decay
		activations_actor = [activation] * (len(actor_layers))
		activations_actor.append('')
		actor_layers: List[int] = [state_size] + actor_layers
		actor_layers.append(self.action_size)
  
		activations_critic = [activation] * (len(critic_layers))
		activations_critic.append('')
		critic_layers: List[int] = [state_size] + critic_layers
		critic_layers.append(1)
  
		self.actor = self._build_model(actor_layers, activations_actor, initializer, initializer_params)
		self.critic = self._build_model(critic_layers, activations_critic, initializer, initializer_params)

		actor_optimizer_params.update(lr=actor_learning_rate)
		self.actor_optimizer = Optimizers.get(actor_optimizer)(self.actor.parameters(), **actor_optimizer_params)

		critic_optimizer_params.update(lr=critic_learning_rate)
		self.critic_optimizer = Optimizers.get(critic_optimizer)(self.critic.parameters(), **critic_optimizer_params)
  
		self.learning_device = "cuda" if t.cuda.is_available() else 'cpu'
		self.eval_device = 'cpu'
		self = self.to(self.learning_device)
		self.I =1
  
	def _build_model(self, layers: List[int], activations: List[str], initializer: str, initializer_params={}):
     
		layer_init = lambda _in, _out, activation=None: nn.Sequential(
			nn.Linear(_in, _out), Activations.get(activation)
		)
  
		qnet = nn.Sequential(*[
	 		layer_init(_in, _out, _activation) for _in, _out, _activation in zip(layers[:-1], layers[1:], activations)
		])
  
	
		for module in qnet.modules():
			if isinstance(module, nn.Linear): 
				Initializers.get(initializer)(module.weight, **initializer_params)
				Initializers.get('const')(module.bias, val=0)

		return qnet
	
	def forward(self, state:t.Tensor):
		return self.actor(state.to(self.learning_device)), self.critic(state.to(self.learning_device))
	
	def get_name(self) -> str:
		return super().get_name() + " - Actor-Critic"
 
	def get_best_action(self, state):
		raw_state = state
		state: t.Tensor =self.parser.parse(state)
		possible_actions = self.get_legal_actions(raw_state)
		with t.no_grad():
			self.eval()
			logits: t.Tensor = self(state)[0].cpu().squeeze(0).gather(0, t.as_tensor(possible_actions))
			
			m, _ = logits.max(-1)
			indices: np.ndarray = t.nonzero(logits == m).numpy().flatten()
		
		cond = indices.size == 1
		action = (possible_actions[indices[0]] if cond else possible_actions[self.rng.choice(indices)])

		return action
	
	def get_action(self, state: Any):
		"""
		Compute the action to take in the current state, basing on policy returned by the network.

		Note: To pick action according to the probability generated by the network
		"""

		#
		# INSERT CODE HERE to get action in a given state
		# 
		logits: t.Tensor
		raw_state = state
		state: t.Tensor =self.parser.parse(state)
		possible_actions = self.get_legal_actions(raw_state)

		with t.no_grad():
			self.eval()
			logits = self(state)[0].cpu().squeeze(0)
			probs: t.Tensor = t.softmax(logits, dim=0)		
		probs_gathered: np.ndarray = probs.gather(0, t.as_tensor(possible_actions)).numpy().flatten() +1e-8
		probs_gathered = probs_gathered/probs_gathered.sum()
		action = self.rng.choice(possible_actions, p=probs_gathered)
		self.last_prob = probs[action].item()
		
		return action

	def learn(self, state, action, reward, next_state, done):
		"""
		Function learn networks using information about state, action, reward and next state. 
		First the values for state and next_state should be estimated based on output of critic network.
		Critic network should be trained based on target value:
		target = r + \gamma next_state_value if not done]
		target = r if done.
		Actor network shpuld be trained based on delta value:
		delta = target - state_value
		"""
		if not isinstance(state, t.Tensor): state= self.parser.parse(state)
		if not isinstance(next_state, t.Tensor): next_state= self.parser.parse(next_state)
		#
		# INSERT CODE HERE to train network
		#
		reward = reward
		gamma = (self.gamma)
		
		value = self.critic(state.to(self.learning_device)).cpu()
		next_value = self.critic(next_state.to(self.learning_device)).cpu() if not done else t.zeros(1)
		delta = (reward + gamma * next_value.item() - value.item())
		critic_loss: t.Tensor =  F.smooth_l1_loss (value.flatten(), reward + gamma * next_value.flatten())
		critic_loss *= self.I
		log_prob: t.Tensor = t.log_softmax(self.actor(state.to(self.learning_device)), dim=1).cpu().flatten().gather(0, t.as_tensor([action], dtype=t.int64))
		actor_loss: t.Tensor = - log_prob * delta
		actor_loss *= self.I
		self.actor_optimizer.zero_grad()
		actor_loss.backward(retain_graph=True)

		self.actor_optimizer.step()
		
		self.critic_optimizer.zero_grad()
		critic_loss.backward()
		self.critic_optimizer.step()
		self.I *= self.loss_decay
		return -actor_loss.detach().item(), critic_loss.detach().item()

